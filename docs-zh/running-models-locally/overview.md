---
title: "本地模型概述"
---

## 在本地运行模型与 Cline

在你自己的硬件上使用真正有能力的模型完全离线运行 Cline。没有 API 成本，没有数据离开你的机器，没有互联网依赖。

本地模型已经达到了一个转折点，现在它们对于实际的开发工作是实用的。本指南涵盖了在本地运行 Cline 所需了解的一切。

## 快速开始

1. **检查你的硬件** - 最低 32GB+ RAM
2. **选择你的运行时** - [LM Studio](/running-models-locally/lm-studio) 或 [Ollama](/running-models-locally/ollama)
3. **下载 Qwen3 Coder 30B** - 推荐的模型
4. **配置设置** - 启用紧凑提示，设置最大上下文
5. **开始编码** - 完全离线

## 硬件要求

你的 RAM 决定了你可以有效运行哪些模型：

| RAM | 推荐模型 | 量化 | 性能级别 |
| --- | --- | --- | --- |
| 32GB | Qwen3 Coder 30B | 4-bit | 入门级本地编码 |
| 64GB | Qwen3 Coder 30B | 8-bit | 完整 Cline 功能 |
| 128GB+ | GLM-4.5-Air | 4-bit | 云竞争性能 |

## 推荐模型

### 主要推荐：Qwen3 Coder 30B

经过广泛测试，**Qwen3 Coder 30B** 是 70B 参数以下对 Cline 最可靠的模型：

- **256K 本机上下文窗口** - 处理整个仓库
- **强大的工具使用能力** - 可靠的命令执行
- **仓库级理解** - 跨文件维护上下文
- **经过验证的可靠性** - 使用 Cline 的工具格式输出一致

下载大小：
- 4-bit：~17GB（推荐用于 32GB RAM）
- 8-bit：~32GB（推荐用于 64GB RAM）
- 16-bit：~60GB（需要 128GB+ RAM）

### 为什么不使用更小的模型？

大多数 30B 参数以下的模型（7B-20B）在 Cline 上失败，因为它们：
- 产生损坏的工具使用输出
- 拒绝执行命令
- 无法维护对话上下文
- 在复杂的编码任务中挣扎

## 运行时选项

### LM Studio
- **优点**：用户友好的 GUI，简单的模型管理，内置服务器
- **缺点**：UI 的内存开销，一次限制为单个模型
- **最适合**：想要简单性的桌面用户
- [设置指南 →](/running-models-locally/lm-studio)

### Ollama
- **优点**：基于命令行，较低的内存开销，可脚本化
- **缺点**：需要终端舒适度，手动模型管理
- **最适合**：高级用户和服务器部署
- [设置指南 →](/running-models-locally/ollama)

## 关键配置

### 必需设置

**在 Cline 中：**
- ✅ 启用"使用紧凑提示" - 将提示大小减少 90%
- ✅ 在设置中设置适当的模型
- ✅ 配置基本 URL 以匹配你的服务器

**在 LM Studio 中：**
- 上下文长度：`262144`（最大）
- KV 缓存量化：`OFF`（对于正确功能至关重要）
- Flash Attention：`ON`（如果你的硬件支持）

**在 Ollama 中：**
- 设置上下文窗口：`num_ctx 262144`
- 如果支持则启用 flash attention

### 理解量化

量化降低模型精度以适应消费者硬件：

| 类型 | 大小减少 | 质量 | 用例 |
| --- | --- | --- | --- |
| 4-bit | ~75% | 好 | 大多数编码任务，有限的 RAM |
| 8-bit | ~50% | 更好 | 专业工作，更多细微差别 |
| 16-bit | 无 | 最好 | 最大质量，需要高 RAM |

### 模型格式

**GGUF（通用）**
- 适用于所有平台（Windows、Linux、Mac）
- 广泛的量化选项
- 更广泛的工具兼容性
- 推荐用于大多数用户

**MLX（仅 Mac）**
- 针对 Apple Silicon (M1/M2/M3) 优化
- 利用 Metal 和 AMX 加速
- 在 Mac 上更快的推理
- 需要 macOS 13+

## 性能预期

### 什么是正常的

- **初始加载时间**：模型预热 10-30 秒
- **Token 生成**：消费者硬件上 5-20 tokens/秒
- **上下文处理**：大型代码库较慢
- **内存使用**：接近你的量化大小

### 性能提示

1. **使用紧凑提示** - 本地推理必不可少
2. **尽可能限制上下文** - 从较小的窗口开始
3. **选择正确的量化** - 平衡质量与速度
4. **关闭其他应用程序** - 为模型释放 RAM
5. **使用 SSD 存储** - 更快的模型加载

## 用例比较

### 何时使用本地模型

✅ **非常适合：**
- 离线开发环境
- 隐私敏感项目
- 无 API 成本的学习
- 无限实验
- 隔离网络环境
- 成本意识强的开发

### 何时使用云模型

☁️ **更适合：**
- 非常大的代码库（>256K tokens）
- 多小时重构会话
- 需要一致性能的团队
- 最新模型功能
- 时间关键项目

## 故障排除

### 常见问题和解决方案

**"Shell 集成不可用"**
- 在 Cline 设置中切换到 bash → 终端 → 默认终端配置文件
- 解决 90% 的终端集成问题

**"无法建立连接"**
- 验证服务器正在运行（LM Studio 或 Ollama）
- 检查基本 URL 是否匹配服务器地址
- 确保没有防火墙阻止连接
- 默认端口：LM Studio (1234)、Ollama (11434)

**缓慢或不完整的响应**
- 本地模型正常（典型 5-20 tokens/秒）
- 尝试更小的量化（4-bit 而不是 8-bit）
- 如果尚未启用紧凑提示则启用
- 减小上下文窗口大小

**模型混淆或错误**
- 验证 KV 缓存量化为 OFF（LM Studio）
- 确保启用了紧凑提示
- 检查上下文长度设置为最大
- 确认量化有足够的 RAM

### 性能优化

**为了更快的推理：**
1. 使用 4-bit 量化
2. 启用 Flash Attention
3. 如果不需要则减小上下文窗口
4. 关闭不必要的应用程序
5. 为模型存储使用 NVMe SSD

**为了更好的质量：**
1. 使用 8-bit 或更高的量化
2. 最大化上下文窗口
3. 确保足够的冷却
4. 为模型分配最大 RAM

## 高级配置

### 多 GPU 设置
如果你有多个 GPU，你可以分割模型层：
- LM Studio：自动 GPU 检测
- Ollama：设置 `num_gpu` 参数

### 自定义模型
虽然推荐 Qwen3 Coder 30B，你可以尝试：
- DeepSeek Coder V2
- Codestral 22B
- StarCoder2 15B

注意：这些可能需要额外的配置和测试。

## 社区和支持

- **Discord**：[加入我们的社区](https://discord.gg/cline)获取实时帮助
- **Reddit**：[r/cline](https://www.reddit.com/r/CLine/)进行讨论
- **GitHub**：[报告问题](https://github.com/cline/cline/issues)

## 下一步

准备好开始了吗？选择你的路径：

<CardGroup cols={2}>
  <Card title="LM Studio 设置" icon="desktop" href="/running-models-locally/lm-studio">
    用户友好的 GUI 方法，带有详细配置指南
  </Card>
  <Card title="Ollama 设置" icon="terminal" href="/running-models-locally/ollama">
    针对高级用户和自动化的命令行设置
  </Card>
</CardGroup>

## 总结

使用 Cline 的本地模型现在真正实用。虽然它们在速度上无法匹配顶级云 API，但它们提供完全的隐私、零成本和离线能力。通过正确的配置和适当的硬件，Qwen3 Coder 30B 可以有效处理大多数编码任务。

关键是正确的设置：足够的 RAM、正确的配置和现实的期望。遵循本指南，你将拥有一个完全在你的硬件上运行的有能力的编码助手。
