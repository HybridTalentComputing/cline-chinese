---
title: "上下文窗口指南"
description: "了解和管理 AI 模型上下文窗口"
---

## 什么是上下文窗口？

上下文窗口是 AI 模型一次可以处理的最大文本量。把它想象成模型的"工作记忆" - 它决定了模型在生成响应时可以考虑多少对话和代码。

<Note>
**关键点**：更大的上下文窗口允许模型一次理解更多代码库，但可能会增加成本和响应时间。
</Note>

## 上下文窗口大小

### 快速参考

| 大小 | Token | 大约单词数 | 使用场景 |
|------|--------|------------------|----------|
| **小** | 8K-32K | 6,000-24,000 | 单个文件、快速修复 |
| **中** | 128K | ~96,000 | 大多数编码项目 |
| **大** | 200K | ~150,000 | 复杂代码库 |
| **超大** | 400K+ | ~300,000+ | 整个应用程序 |
| **巨大** | 1M+ | ~750,000+ | 多项目分析 |

### 模型上下文窗口

| 模型 | 上下文窗口 | 有效窗口* | 备注 |
|-------|---------------|------------------|-------|
| **Claude Sonnet 4.5** | 1M tokens | ~500K tokens | 在高上下文时质量最佳 |
| **GPT-5** | 400K tokens | ~300K tokens | 三种模式影响性能 |
| **Gemini 2.5 Pro** | 1M+ tokens | ~600K tokens | 非常适合文档 |
| **DeepSeek V3** | 128K tokens | ~100K tokens | 对大多数任务最优 |
| **Qwen3 Coder** | 256K tokens | ~200K tokens | 良好的平衡 |

*有效窗口是模型保持高质量的范围

## 高效管理上下文

### 什么计入上下文

1. **你当前的对话** - 聊天中的所有消息
2. **文件内容** - 你分享的任何文件或 Cline 已读取的文件
3. **工具输出** - 执行命令的结果
4. **系统提示** - Cline 的指令（影响最小）

### 优化策略

#### 1. 为新功能重新开始
```
/new - 创建具有干净上下文的新任务
```
好处：
- 最大可用上下文
- 无不相关历史
- 更好的模型焦点

#### 2. 策略性地使用 @ 提及
而不是包含整个文件：
- `@filename.ts` - 仅在需要时包含
- 使用搜索而不是读取大文件
- 引用特定函数而不是整个文件

#### 3. 启用自动压缩
Cline 可以自动总结长对话：
- 设置 → 功能 → 自动压缩
- 保留重要上下文
- 减少 token 使用

## 上下文窗口警告

### 达到限制的迹象

| 警告信号 | 含义 | 解决方案 |
|-------------|---------------|----------|
| **"上下文窗口超出"** | 达到硬限制 | 开始新任务或启用自动压缩 |
| **响应变慢** | 模型上下文处理困难 | 减少包含的文件 |
| **重复建议** | 上下文碎片化 | 总结并重新开始 |
| **缺少最近的更改** | 上下文溢出 | 使用检查点跟踪更改 |

### 按项目大小的最佳实践

#### 小型项目（< 50 个文件）
- 任何模型都工作良好
- 自由包含相关文件
- 不需要特殊优化

#### 中型项目（50-500 个文件）
- 使用 128K+ 上下文模型
- 仅包含工作文件集
- 在功能之间清除上下文

#### 大型项目（500+ 个文件）
- 使用 200K+ 上下文模型
- 专注于特定模块
- 使用搜索而不是读取许多文件
- 将工作分解为更小的任务

## 高级上下文管理

### 计划/执行模式优化

利用计划/执行模式获得更好的上下文使用：
- **计划模式**：使用较小的上下文进行讨论
- **执行模式**：包含必要的文件进行实施

配置：
```
计划模式：DeepSeek V3 (128K) - 低成本规划
执行模式：Claude Sonnet (1M) - 最大编码上下文
```

### 上下文修剪策略

1. **时间修剪**：移除旧的对话部分
2. **语义修剪**：仅保留相关代码部分
3. **层次修剪**：维护高级结构，修剪细节

### Token 计数技巧

#### 粗略估计
- **1 token ≈ 0.75 个单词**
- **1 token ≈ 4 个字符**
- **100 行代码 ≈ 500-1000 tokens**

#### 文件大小指南
| 文件类型 | 每 KB 的 Tokens |
|-----------|---------------|
| **代码** | ~250-400 |
| **JSON** | ~300-500 |
| **Markdown** | ~200-300 |
| **纯文本** | ~200-250 |

## 上下文窗口常见问题

### 问：为什么非常长的对话响应变差？
**答：** 模型可能会在太多上下文时失去焦点。"有效窗口"通常是宣传限制的 50-70%。

### 问：我应该使用可用的最大上下文窗口吗？
**答：** 不一定。更大的上下文会增加成本并可能降低响应质量。根据任务大小匹配上下文。

### 问：我如何知道我使用了多少上下文？
**答：** Cline 在界面中显示 token 使用情况。注意上下文仪表接近限制。

### 问：当我超过上下文限制时会发生什么？
**答：** Cline 将要么：
- 自动压缩对话（如果启用）
- 显示错误并建议开始新任务
- 截断旧消息（带有警告）

## 按用例推荐

| 用例 | 推荐上下文 | 模型建议 |
|----------|-------------------|------------------|
| **快速修复** | 32K-128K | DeepSeek V3 |
| **功能开发** | 128K-200K | Qwen3 Coder |
| **大型重构** | 400K+ | Claude Sonnet 4.5 |
| **代码审查** | 200K-400K | GPT-5 |
| **文档** | 128K | 任何预算模型 |
